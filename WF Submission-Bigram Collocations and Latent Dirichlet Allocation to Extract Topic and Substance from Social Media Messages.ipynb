{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, brown\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "import lda\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_corpus(file):\n",
    "    with open(file) as f:\n",
    "        raw_text = f.read()\n",
    "    return raw_text\n",
    "\n",
    "def tokenize_sents(raw_text):\n",
    "    raw_sents = nltk.sent_tokenize(raw_text)\n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def tokenize_words(raw_text):\n",
    "    return nltk.word_tokenize(raw_text)\n",
    "\n",
    "def create_data_sets():\n",
    "    tagged_sents = brown.tagged_sents(categories='news')\n",
    "    size = int(len(tagged_sents) * 0.9)\n",
    "    train_sents = tagged_sents[:size]\n",
    "    test_sents = tagged_sents[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def create_backoff_tagger(): \n",
    "    train_sents, test_sents = create_data_sets()\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from CSV to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>MediaType</th>\n",
       "      <th>FullText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AutoID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/26/2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>twitter</td>\n",
       "      <td>3 ways the internet of things will change Bank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/5/2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>twitter</td>\n",
       "      <td>BankB BankB Name downgrades apple stock to neu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/12/2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>twitter</td>\n",
       "      <td>BankB returns to profit on INTERNET/! board2? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/5/2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>twitter</td>\n",
       "      <td>BankB tells advisers to exit paulson hedge fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8/12/2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>twitter</td>\n",
       "      <td>BankC may plead guilty over foreign exchange p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Year  Month MediaType  \\\n",
       "AutoID                                     \n",
       "1       8/26/2015  2015      8   twitter   \n",
       "2        8/5/2015  2015      8   twitter   \n",
       "3       8/12/2015  2015      8   twitter   \n",
       "4        8/5/2015  2015      8   twitter   \n",
       "5       8/12/2015  2015      8   twitter   \n",
       "\n",
       "                                                 FullText  \n",
       "AutoID                                                     \n",
       "1       3 ways the internet of things will change Bank...  \n",
       "2       BankB BankB Name downgrades apple stock to neu...  \n",
       "3       BankB returns to profit on INTERNET/! board2? ...  \n",
       "4       BankB tells advisers to exit paulson hedge fun...  \n",
       "5       BankC may plead guilty over foreign exchange p...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dataset.txt\", delimiter=\"|\", encoding=\"latin-1\", index_col=\"AutoID\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opted to focus primarily on the Facebook data for a few reasons:\n",
    "* It covers a longer time period, 12 months versus one month\n",
    "* The messages were longer and more resembled coherent, english thoughts\n",
    "* We did not have the ability to find Twitter messages that spanned across several tweets by a single user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Filter to Facebook messages only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "facebook = dataset[dataset['MediaType'] == 'facebook']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify stopwords and remove irrelevant messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>MediaType</th>\n",
       "      <th>FullText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AutoID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>facebook</td>\n",
       "      <td>laude Name BankB  in the line up when the# fly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1/9/2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>facebook</td>\n",
       "      <td>- any body banking with BankC bank bank tryna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>8/19/2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>facebook</td>\n",
       "      <td>- bitches be thoting off Name Name me Name Nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10/12/2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>facebook</td>\n",
       "      <td>- center for aligned healing special announcem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>8/20/2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>facebook</td>\n",
       "      <td>- facebook wife wife?   :- *      ¥ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date  Year  Month MediaType  \\\n",
       "AutoID                                      \n",
       "56       9/30/2014  2014      9  facebook   \n",
       "65        1/9/2015  2015      1  facebook   \n",
       "66       8/19/2014  2014      8  facebook   \n",
       "67      10/12/2014  2014     10  facebook   \n",
       "69       8/20/2014  2014      8  facebook   \n",
       "\n",
       "                                                 FullText  \n",
       "AutoID                                                     \n",
       "56      laude Name BankB  in the line up when the# fly...  \n",
       "65      - any body banking with BankC bank bank tryna ...  \n",
       "66      - bitches be thoting off Name Name me Name Nam...  \n",
       "67      - center for aligned healing special announcem...  \n",
       "69      - facebook wife wife?   :- *      ¥ ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83647, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the most commonly occurring tokens in the messages with a very basic FreqDist, we identified two lists of Stop Words. The first, filter_stop, consisting of words that had been added to dataset by Wells Fargo as masks. The second, addl_stop, were words and characters we identified as occurring with very high frquency in messages that were of no analytical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_stop = {'Name', 'INTERNET', 'BankA', 'BankB', 'BankC', 'BankD',\n",
    "              'BankAs', 'BankBs', 'BankCs', 'BankDs','bankd', 'bankds',\n",
    "              'internet', 'bankbs', 'bankb', 'bankcs', 'name', 'banka',\n",
    "              'bankc', 'bankas'}\n",
    "# additional stopwords derived from examining 100 most common tokens in dataset -- Signifiers of OWS movement, largely\n",
    "addl_stop = {'â', 'giannis', 'banksters', 'classwarfare', 'financialterrorists', \n",
    "             'morganstanley', 'vote', 'banke', 'BankE', 'bankE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exclude_stop_items(row, addl_stop):\n",
    "    words = tokenize_words(row)\n",
    "    if [x for x in words if x in addl_stop]:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_stop = facebook['FullText'].apply(lambda x: exclude_stop_items(x, addl_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding these items reduced the size of the dataset by about 17%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70209, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook[no_stop].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Topic Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next used a few methods to explore the remaining data for key concepts to focus on in the second half of the analysis. Below we look at most commonly occurring tokens, as well as frequently occurring bigrams and trigrams, then apply some filtering based on part-of-speech patterns we identify as fruitful. For this section, we take about half of the total messages out of the Pandas DataFrame and into a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = ' '.join(facebook[no_stop]['FullText'].tolist())[:7000000] # Half the dataset, roughly, for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = tokenize_words(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_stop_list = [x for x in words if x not in stopwords.words('english') and x.isalpha()\n",
    "              and x not in filter_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bank', 10672),\n",
       " ('account', 5290),\n",
       " ('get', 3332),\n",
       " ('money', 3177),\n",
       " ('new', 2822),\n",
       " ('like', 2423),\n",
       " ('one', 2319),\n",
       " ('got', 2288),\n",
       " ('card', 2242),\n",
       " ('financial', 2202)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(no_stop_list)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create bigrams from messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the most common tokens are interesting, they are not giving us anything specific to zero in on, so we looked at the top bigrams in terms of PMI and raw frequency. Please note, the next section runs fairly slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(no_stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate most common bigrams by PMI and raw frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_measures = BigramAssocMeasures()\n",
    "top_pmi = finder.nbest(bigram_measures.pmi, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_raw = finder.nbest(bigram_measures.raw_freq, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BankAales', 'sickens'),\n",
       " ('BankAcimaginedragons', 'imaginedragons'),\n",
       " ('BankAon', 'gianormous'),\n",
       " ('BankCcreditcardmemberprivilege', 'koskinlaser'),\n",
       " ('BankCfoundation', 'financialstability'),\n",
       " ('BankCthankyoucards', 'ajhudson'),\n",
       " ('BankDdesignation', 'develeporexperience'),\n",
       " ('BankDhashumanbeingemployees', 'byebyecreditcards'),\n",
       " ('BankDhomefinance', 'thanksmonique'),\n",
       " ('BankDisntinboston', 'BankDappredesignfail')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pmi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('financial', 'advisers'),\n",
       " ('wealth', 'managers'),\n",
       " ('advisers', 'wealth'),\n",
       " ('asset', 'financial'),\n",
       " ('bank', 'bank'),\n",
       " ('chicago', 'marathon'),\n",
       " ('customer', 'service'),\n",
       " ('bank', 'account'),\n",
       " ('debit', 'card'),\n",
       " ('shared', 'photo')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_raw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI is finding too specific of topics, but the top raw bigrams seems to be working very well. We hand identified the best topics from here, then looked back to see if they shared a common tag pattern that could be used to generalize into a list of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger = create_backoff_tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('financial', 'JJ'), ('advisers', 'NNS')],\n",
       " [('wealth', 'NN'), ('managers', 'NNS')],\n",
       " [('advisers', 'NNS'), ('wealth', 'NN')],\n",
       " [('asset', 'NN'), ('financial', 'JJ')],\n",
       " [('bank', 'NN'), ('bank', 'NN')],\n",
       " [('chicago', 'NN'), ('marathon', 'NN')],\n",
       " [('customer', 'NN'), ('service', 'NN')],\n",
       " [('bank', 'NN'), ('account', 'NN')],\n",
       " [('debit', 'NN'), ('card', 'NN')],\n",
       " [('shared', 'VBD'), ('photo', 'NN')],\n",
       " [('breaking', 'VBG'), ('news', 'NN')],\n",
       " [('rating', 'NN'), ('reiterated', 'VBN')],\n",
       " [('would', 'MD'), ('like', 'VB')],\n",
       " [('center', 'NN'), ('center', 'NN')],\n",
       " [('credit', 'NN'), ('card', 'NN')]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_bgs = []\n",
    "for bg in top_raw:\n",
    "    tagged_bgs.append(tagger.tag(bg))\n",
    "    \n",
    "tagged_bgs[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Identify common part-of-speech patterns in useful topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_patterns = {('JJ', 'NN'),\n",
    "                 ('JJ', 'NNS'),\n",
    "                 ('JJT', 'NN'),\n",
    "                 ('NN', 'NN'),\n",
    "                 ('NN', 'NNS'),\n",
    "                 ('NNS', 'VB')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Find bigram collocations based on frequency and POS pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('financial', 'JJ'), ('advisers', 'NNS')],\n",
       " [('wealth', 'NN'), ('managers', 'NNS')],\n",
       " [('chicago', 'NN'), ('marathon', 'NN')],\n",
       " [('customer', 'NN'), ('service', 'NN')],\n",
       " [('bank', 'NN'), ('account', 'NN')],\n",
       " [('debit', 'NN'), ('card', 'NN')],\n",
       " [('credit', 'NN'), ('card', 'NN')],\n",
       " [('neutral', 'JJ'), ('rating', 'NN')],\n",
       " [('checking', 'NN'), ('account', 'NN')],\n",
       " [('data', 'NN'), ('breach', 'NN')],\n",
       " [('goldman', 'NN'), ('sachs', 'NN')],\n",
       " [('overweight', 'NN'), ('rating', 'NN')],\n",
       " [('gon', 'NN'), ('na', 'NN')],\n",
       " [('good', 'JJ'), ('morning', 'NN')],\n",
       " [('new', 'JJ'), ('photos', 'NN')],\n",
       " [('wan', 'NN'), ('na', 'NN')],\n",
       " [('marathon', 'NN'), ('chicago', 'NN')],\n",
       " [('close', 'JJ'), ('account', 'NN')],\n",
       " [('bank', 'NN'), ('robbery', 'NN')],\n",
       " [('small', 'JJ'), ('business', 'NN')],\n",
       " [('new', 'JJ'), ('bank', 'NN')],\n",
       " [('account', 'NN'), ('bank', 'NN')],\n",
       " [('financial', 'JJ'), ('crisis', 'NN')],\n",
       " [('worst', 'JJT'), ('bank', 'NN')],\n",
       " [('money', 'NN'), ('account', 'NN')],\n",
       " [('tickets', 'NNS'), ('see', 'VB')],\n",
       " [('cant', 'NN'), ('wait', 'NN')],\n",
       " [('new', 'JJ'), ('photo', 'NN')],\n",
       " [('real', 'JJ'), ('estate', 'NN')],\n",
       " [('credit', 'NN'), ('cards', 'NN')],\n",
       " [('carolina', 'NN'), ('panthers', 'NN')],\n",
       " [('cash', 'NN'), ('check', 'NN')],\n",
       " [('small', 'JJ'), ('businesses', 'NNS')],\n",
       " [('good', 'JJ'), ('luck', 'NN')],\n",
       " [('direct', 'JJ'), ('deposit', 'NN')],\n",
       " [('online', 'NN'), ('banking', 'NN')],\n",
       " [('account', 'NN'), ('number', 'NN')],\n",
       " [('tour', 'NN'), ('center', 'NN')],\n",
       " [('bank', 'NN'), ('card', 'NN')],\n",
       " [('price', 'NN'), ('target', 'NN')],\n",
       " [('open', 'JJ'), ('account', 'NN')],\n",
       " [('savings', 'NNS'), ('account', 'VB')],\n",
       " [('asset', 'NN'), ('management', 'NN')],\n",
       " [('memorial', 'NN'), ('fund', 'NN')],\n",
       " [('united', 'NN'), ('states', 'NNS')],\n",
       " [('high', 'JJ'), ('school', 'NN')],\n",
       " [('money', 'NN'), ('bank', 'NN')],\n",
       " [('credit', 'NN'), ('union', 'NN')]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_tagged_bgs = []\n",
    "for bg in top_raw:\n",
    "    tagged = tagger.tag(bg)\n",
    "    pattern = (tagged[0][1], tagged[1][1])\n",
    "    if pattern in good_patterns and tagged[0] != tagged[1]:\n",
    "        good_tagged_bgs.append(tagger.tag(bg))\n",
    "    \n",
    "good_tagged_bgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_tagged_bgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the tagging reduced our total list to 48, a manageable number to see what additional topics may be relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cross Tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify which topic is being discussed in each message and add to DataFrame\n",
    "\n",
    "Further analysis on groups of messages containing these bigrams proves challenging because the main takeaway is that all of them contain the same bigrams. We added columns to our dataframes to\n",
    "1. Make it easier to access the messages from a given topic\n",
    "2. Filter out the actual topic being referred to, in order to make it easier to analyze the substance of the rest of the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_column(row, bigram):\n",
    "    bg = [x[0] for x in bigram] # Remove POS tags\n",
    "    if ' '.join(bg) in row:\n",
    "        words = tokenize_words(row.replace(' '.join(bg), ''))\n",
    "        tagged_words = tagger.tag(words)\n",
    "        return ' '.join([x[0] for x in tagged_words if x[0] not in filter_stop])\n",
    "                        \n",
    "    elif ''.join(bg) in row:\n",
    "        words = tokenize_words(row.replace(''.join(bg), ''))\n",
    "        tagged_words = tagger.tag(words)\n",
    "        return ' '.join([x[0] for x in tagged_words if x[0] not in filter_stop])\n",
    "    else:\n",
    "        return False\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulglenn/iSchool/256/Anaconda/anaconda/lib/python3.4/site-packages/IPython/kernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "for bigram in good_tagged_bgs:\n",
    "    bg = '_'.join([x[0] for x in bigram]) # Column name is bigram separated by _\n",
    "    facebook[bg] = facebook['FullText'].apply(add_column, args=(bigram,))\n",
    "    topics.append(bg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Identify which bank is being discussed in each message and add to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_topics = []\n",
    "for item in good_tagged_bgs:\n",
    "    good_topics.append(item[0][0] + \"_\" + item[1][0])\n",
    "\n",
    "topics_of_interest = good_topics\n",
    "fb_subset = facebook.ix[:,[\"FullText\"] + topics_of_interest]\n",
    "for topic in topics_of_interest:\n",
    "    fb_subset[\"num_\" + topic] = fb_subset[topic].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "    \n",
    "def find_bank(in_str):\n",
    "    banks = {\n",
    "            \"bank_a\": [\"twit_hndl_BankA\", \"BankA\"],\n",
    "            \"bank_b\": [\"twit_hndl_BankB\", \"BankB\"],\n",
    "            \"bank_c\": [\"twit_hndl_BankC\", \"BankC\"],\n",
    "            \"bank_d\": [\"twit_hndl_BankD\", \"BankD\"],\n",
    "    }\n",
    "    \n",
    "    # assumes only one bank per comment\n",
    "    for bank, idents in banks.items():\n",
    "        for ident in idents:\n",
    "            if ident.lower() in in_str.lower():\n",
    "                return bank\n",
    "    \n",
    "    return None\n",
    "\n",
    "fb_subset[\"bank\"] = fb_subset[\"FullText\"].apply(lambda x: find_bank(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cross-tabulate messages by topic and bank and graphical analysis in Tableau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fb_subset.to_csv(\"viz_data_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Substance Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since this code is fairly slow, we are only running it here \n",
    "# against the hand-drawn list of topics used in our write-up\n",
    "revised_topics = ['financial_advisers',\n",
    "                  'wealth_managers',\n",
    "                  'customer_service',\n",
    "                  'bank_account',\n",
    "                  'debit_card',\n",
    "                  'credit_card',\n",
    "                  'checking_account',\n",
    "                  'data_breach',\n",
    "                  'bank_robbery',\n",
    "                  'small_business',\n",
    "                  'real_estate',\n",
    "                  'close_account',\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "financial_advisers\n",
      "wealth_managers\n",
      "customer_service"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bank_account"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "debit_card"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "credit_card"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "checking_account"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data_breach"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bank_robbery"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "small_business\n",
      "real_estate"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "close_account"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysis inspired by http://aylien.com/web-summit-2015-tweets-part1\n",
    "\n",
    "substances = {} # Dictionary to store the top unique words from each cluster for each topic\n",
    "\n",
    "for topic in revised_topics: # To run against the full list, iterate over topics instead of revised_topics\n",
    "    print(topic)\n",
    "    substances[topic] = {}\n",
    "    \n",
    "    data = facebook[facebook[topic] != False] # Get the rows that correspond with this topic\n",
    "    \n",
    "    # Convert messages within topic to matrix of token counts\n",
    "    cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')\n",
    "    cvz = cvectorizer.fit_transform(data[topic])\n",
    "\n",
    "    # Fit LDA clustering model\n",
    "    n_topics = 10\n",
    "    n_iter = 3000\n",
    "    lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "    X_topics = lda_model.fit_transform(cvz)\n",
    "    \n",
    "    \n",
    "    # Iterate over the clusters created within the topic and find the distinguishing words \n",
    "    n_top_words = 15\n",
    "    topic_words = lda_model.topic_word_  \n",
    "    vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "    # Add the distinguishing words to a list\n",
    "    topic_summaries = []\n",
    "    for i, topic_dist in enumerate(topic_words):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        topic_summaries.append(' '.join(topic_words))\n",
    "    \n",
    "    \n",
    "    # Flatten all topic descriptors to bag of words\n",
    "    all_words = ' '.join(x for x in topic_summaries) \n",
    "    counts = Counter(all_words.split(' ')) # Create a counter of words in the bag\n",
    "    \n",
    "    # Add unique summary words to dictionary to better identify each topic by the distinguishing\n",
    "    # words it does not share with other topic descriptions\n",
    "    for i, summary in enumerate(topic_summaries):\n",
    "        substances[topic][i] = [x for x in summary.split(' ') if counts[x] == 1] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note: The dictionary of topics and substances can be viewed in the variable substances or by exporting to csv below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output substances to csv for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf = pd.DataFrame.from_dict(substances).transpose().stack()\n",
    "sf.to_csv('output.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
